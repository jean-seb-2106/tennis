---
title: "module6"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{module6}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup}
library(asta)
library(skimr)
library(tidymodels)
library(ggthemes)
library(ranger)
library(kknn)
library(purrr)
library(aws.s3)
library(readxl)
```



# I - menuItem : Classification supervisée

## I - 1 menuSubItem : données

Choisir la base de données et le pourcentage de test :

```{r}
# Grandile
# data <- grandile   #changer la base de données ("vins","grandile")
# data <- data %>% rename(target = PAUVRE) %>%
#   select(-starts_with("LIB")) %>%
#   select(-IDENT) %>%
#   mutate(target = as.factor(target))

#vins
# data <- vins %>% rename(target = quality)


#tennis base de données : premier nettoyage des données

#connexion à la base de données S3 et récupération de la BDD tennis
aws.s3::get_bucket("jean/tennis", region = "")
BUCKET <- "jean"
FILE_KEY_S3 <- "tennis/2023.xlsx"
data <- 
  aws.s3::s3read_using(
    FUN = readxl::read_xlsx,
    # Mettre les options de FUN ici
    # delim = ";",
    object = FILE_KEY_S3,
    bucket = BUCKET,
    opts = list("region" = "")
  )

# data <- readxl::read_xlsx("C:/Users/XX9JQZ/Downloads/archive(3)/atp_mens_tour/2023.xlsx")


#explo des données
# skim(data)

recette_data <- recipe(~., data = data) %>%
  step_mutate_at(c(WRank,LRank,WPts,LPts),fn = as.numeric) %>% #transfo en numérique
  step_naomit(WRank, LRank,B365W,PSW,MaxW,AvgW) %>% #enlever les individus avec des valeurs manquantes
  step_rm(W1,L1,W2,L2,W3,L3,W4,L4,W5,L5,Wsets,Lsets) #enlever des variables

data_modif <- prep(recette_data) %>%
  bake(new_data = NULL) 
# skim(data_modif)


#si le joueur est mieux classé, alors c'est le joueur 1 
data_modif <- data_modif %>% mutate(j1 = if_else(WRank > LRank,Winner,Loser),
                                    j2 = if_else(WRank < LRank,Winner,Loser),
                                    target = if_else(j1 == Winner,1,0)
                                    )


skim(data)
```


En affichage : 
le skim du train (fichier d'entraînement)
le skim du test (fichier test)

En variable globale, on a les bases de données train et test qui seront 
transmises aux sous-modules suivants.

```{r}
#paramètres



part_training <- 0.6 #proportion du découpage aléatoire en training et test
part_validation <- 0.2
var_strata <- "target" #variable de stratification : souvent la variable target

# set.seed(123)
#Découpage en training et en test (ancien découpage)
# data_split <- rsample::initial_split(data,
                                     # strata = .data[[var_strata]],
                                     # prop = part_training)
data_split <- initial_validation_split(data,strata = .data[[var_strata]],prop = c(part_training,part_validation))
# data_split <- initial_validation_split(data,strata = all_of(var_strata))

train_data <- training(data_split) #le fichier d'entraînement
test_data <- testing(data_split) #le fichier de test
valid_data <- validation(data_split)
train_valid_data <- train_data %>% bind_rows(valid_data)

# skim(train_valid_data)
# skim(test_data)

```



## I - 2 menuSubItem : preparation de la base

On part du fichier d'entraînement qui est une variable globale du module précédent.

### Modifs de la base d'entraînement

Une fenêtre avec la modif de la base de données : 
A la fin du module de préparation, la base de données a été modifiée : 
- selection des variables rentrant dans le modèle
- centrage-réduction d'une sélection de variables numériques
- pour transformer des variables nominales en indicatrices
- pour mettre certaines variables au carré ou en interaction
On obtient donc une recette après sélection des différents items et un clic. 


```{r}
#################### 1 - Recettes #####################################----------------

#1 - Création de la recette : on met toutes les variables dans un premier temps
#Voulez vous retirer des variables du modèle ? (liste des variables)
#Voulez-vous centrer réduire les variables ? (oui/non)

#Modèle avec toutes les variables
rec1 <- 
  recipe(target ~ ., data = train_valid_data) 

#Modèle avec deux variables en moins
rec2 <- 
  recipe(target ~ ., data = train_valid_data) %>% 
  step_rm(c(fixed.acidity,volatile.acidity))

#Modèle avec toutes les variables centrées réduites
rec3 <- 
  recipe(target ~ ., data = train_valid_data) %>%
  step_normalize(all_numeric_predictors()) 
# %>% step_dummy(all_nominal_predictors()) %>% #: pour transformer les variables nominales en indicatrices
# %>% update_role(flight, time_hour, new_role = "ID") %>% #: pour retirer des variables du modèle
# %>%step_normalize(all_numeric_predictors()) #pour centrer réduire
# %>% step_zv() #pour enlever les variables avec une seules valeur
# %>% step_rm() #removes variables
# %>% step_impute_mode() #imputation des valeurs manquantes avec le mode
# %>% step_impute_mean() #imputation des valeurs manquates avec la moyenne
# %>% step_clean_names #nettoyer le nom des variables

#Recette pour grandile (vin n'a pas de qualitatives)
rec4 <- 
  recipe(target ~ ., data = train_valid_data) %>%
  # step_rm(all_nominal_predictors())   %>% 
  step_normalize(all_numeric_predictors()) %>%
  # step_string2factor(target) %>% 
  # step_rm(IDENT) %>% 
  step_dummy(all_nominal_predictors()) 

#la recette choisie parmi les recettes ci-dessous quand on clique sur validé
rec <- rec4 #paramètre à changer pour changer de recette
rm(rec1,rec2,rec3,rec4)#pour nettoyer l'environnement
```

Affichage de la table tranformée après la recette

```{r}
data_rec <- bake(prep(rec),new_data = NULL)
skim(data_rec)
```

## I - 3 menuSubItem : Choix du modèle/algo


Paramètre algo : 
Une autre fenêtre avec le choix de l'algorithme : 
- la regression logistique
- l'arbre (hyparamètres par défaut)
- le KNN (hyparamètres par défaut)
- la forêt (hyparamètres par défaut)
On obtient un modèle (à voir si on optimise les hyper-paramètres)

```{r}

##############         2 - Modèles      ########################-----------------

#2 - Choix de l'algorithme :

#2-1 la regression logistique
mod_lr <- 
  logistic_reg() %>% 
  set_engine("glm")

#2-2 la forêt aléatoire
#Les paramètres de la forêt aléatoire : 
# trees nombre d'arbres trees
# Nombre de variables pour chaque arbre
mod_rf <- 
  rand_forest(trees = 1000,
              mtry = 3,
              min_n = NULL) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")


#3-3 l'arbre de décision

mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.001,
    tree_depth = 7,
    min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("classification")


# mod_tree <- 
#   decision_tree(
#     cost_complexity = tune(),
#     tree_depth = tune(),
#     min_n = NULL
#   ) %>% 
#   set_engine("rpart") %>% 
#   set_mode("classification")
# #Hyper-paramètres à tester, généré automatiquement par grid regular
# grid_tree <- grid_regular(cost_complexity(),
#                           tree_depth(),
#                           levels = 5)
#Je teste 25 combinaisons d'hyper paramètres pour trouver le meilleur arbre

#3-4 KNN
mod_knn <- 
  nearest_neighbor(
    neighbors = 3
  ) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")


#3-5 la regression lasso
mod_lasso <- 
  logistic_reg(penalty = 0.001, 
               mixture = 1) %>% 
  set_engine("glmnet")
#lasso grid : 30 hyper-paramètres à tester
grid_lasso <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

#3-4 la regression ridge
mod_ridge <- 
  logistic_reg(penalty = tune(), 
               mixture = 0) %>% 
  set_engine("glmnet")
#ridge grid : 30 hyper-paramètres à tester
grid_ridge <- tibble(penalty = 10^seq(-4, -1, length.out = 30))

#SVM : machine à support de vecteur
mod_svm <- svm_rbf(mode = "classification", 
                     cost = 10, 
                     rbf_sigma = 0.1, 
                     margin = 1) %>%
  set_engine("kernlab")

#On choisit la recette
mod <- mod_lr #paramètre à changer pour changer de modèle

rm(mod_lr,mod_ridge,mod_lasso,mod_rf,mod_tree,mod_knn,mod_svm)
```


```{r}
############# 3 - Workflows     ########################--------------
#3 - Création des workflows

#Workflow sans tuning
wflow <-  workflow() %>% 
  add_model(mod) %>% #ajout du modèle
  add_recipe(rec) #ajout de la recette (transfo de la base initiale)
wflow
```

```{r}
#Ajustement et affichage du résultat du modèle

fit <- wflow %>% fit(data=train_data)

fit %>% 
 extract_fit_parsnip()

```

```{r}
#estimations sur la base d'entraînement

augment(fit,train_data) %>% select(target,starts_with(".pred"))

```

## I-4 menu Subitem : Validation


### Validation croisée

On joint la recette et le modèle pour arriver à un workflow, qui est la 
variable globale de ce module qu'on va utiliser dans le sous-module suivant. 

Dans ce module, on affiche la base d'entraînement (qui peut être modifiée) et le modèle choisi + son nom.

On ne fait que la validation croisée sur les données d'entraînement. On peut changer le nombre de folds.
On choisit un workflow dans la liste (avec une recette et un modèle) et on affiche les résultats.
ça peut prendre plus ou moins de temps selon l'alogorithme choisi. 



### Validation simple

```{r}
predict(fit,new_data = valid_data)
pred_valid <- augment(fit, valid_data) %>% select(target,starts_with(".pred"))
pred1 <- names(pred_valid)[3] #récupération du nom de la variable qui donne la première proba 

roc_plot_valid <- pred_valid %>% 
  roc_curve(truth = target, .data[[pred1]]) %>% 
  autoplot()
roc_plot_valid

#Aire sous la courbe
pred_valid %>% 
  roc_auc(truth = target, .data[[pred1]])

#Accuracy : pourcentage de biens classés
pred_valid %>% 
  accuracy(truth = target, .pred_class)

#Spécificité
pred_valid %>% 
  specificity(truth = target, .pred_class)

#Sensitivité
pred_valid %>% 
  sensitivity(truth = target, .pred_class)

pred_valid %>%
  conf_mat(target, .pred_class) %>%
  autoplot(type="heatmap")
  
  # pluck(1) %>%
  # as_tibble() %>%
  # ggplot(aes(Prediction, Truth, fill = n)) +
  # geom_tile(show.legend = FALSE) +
  # geom_text(aes(label = n), colour = "white", alpha = 1, size = 5) +
  # ggtitle("Table de confusion") +
  # xlab("Classe prédite") + ylab("Classe réelle") +
  # theme_hc() + scale_colour_hc()
```


```{r}
##############Validation croisée##################
# set.seed(345)
nb_folds <- 5
folds <- vfold_cv(train_valid_data,
                  v = nb_folds)#paramétrage de la validation croisée
# 
# Evaluation du modèle avec la validation croisée
metrics <- metric_set(accuracy,recall, precision,roc_auc,sensitivity, specificity)
fit_rs <- wflow %>% fit_resamples(folds,
                                  metrics = metrics,
                                  control = control_resamples(save_pred = TRUE))

nb_rs_metrics <- collect_metrics(fit_rs)
nb_rs_predictions <- collect_predictions(fit_rs)

nb_rs_predictions %>% 
  accuracy(truth = target, .pred_class)

#Dans les infoBoxs
accuracy <- nb_rs_predictions %>% accuracy(truth = target, .pred_class) %>% select(.estimate) %>% round(2) %>% as.character()
sensitivity <- nb_rs_predictions %>% sensitivity(truth = target, .pred_class) %>% select(.estimate) %>% round(2) %>% as.character()
specificity <- nb_rs_predictions %>% specificity(truth = target, .pred_class) %>% select(.estimate) %>% round(2) %>% as.character()

roc_auc <- nb_rs_metrics[nb_rs_metrics$.metric=="roc_auc","mean"] %>% as.character()

##dans le premier renderPlot
fit_rs %>% collect_metrics()
conf_mat_resampled(fit_rs, tidy = FALSE) %>%
  autoplot(type = "heatmap")

##dans le deuxième renderPlot
nb_rs_predictions %>% 
  roc_curve(truth = target, .pred_bon) %>% 
  autoplot()

```

A la fin de cette étape, on a choisi un modèle : on prend le meilleur au vu des résultats et on clique sur OK. 
c'est ce modèle qui sera utilisé pour la généralisation sur la phase de test. 

## I -4 menuSubItem : généralisation

En entrée, on a le modèle qui a été sélectionné.

```{r}
#5 - Visualisation du résultat (regression logistique et random forest)

fit_final <- 
  wflow %>% 
  fit(data = train_valid_data) 
  
#6 - Prédiction sur la base de test
pred_testing <- augment(fit_final, test_data) #renvoie une base avec aussi les probas 


#affichage de la courbe ROC
roc_plot_testing <- pred_testing %>% 
  roc_curve(truth = target, .pred_bon) %>% 
  autoplot()
roc_plot_testing

#Aire sous la courbe
pred_testing %>% 
  roc_auc(truth = target, .pred_bon)

#Accuracy : pourcentage de biens classés
pred_testing %>% 
  accuracy(truth = target, .pred_class)

#Spécificité
pred_testing %>% 
  specificity(truth = target, .pred_class)

#Sensitivité
pred_testing %>% 
  sensitivity(truth = target, .pred_class)

pred_testing %>%
  conf_mat(target, .pred_class) %>%
  pluck(1) %>%
  as_tibble() %>%
  ggplot(aes(Prediction, Truth, fill = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = "white", alpha = 1, size = 5) +
  ggtitle("Table de confusion") +
  xlab("Classe prédite") + ylab("Classe réelle") +
  theme_hc() + scale_colour_hc()

```


On a sélectionné le meilleur modèle et on teste ses performances sur des données qu'il n'a jamais vu. 
On fait apparaître le tableau de truc + courve AUC + Accuracy (pour voir le taux de biens classés par exemple).

# II - menuItem : Regression

## II - 1 menuSubitem : données

L'utilisateur choisit la base entre ozone et grandile dans le menu déroulant. 

```{r}

data <- ozone %>%
  mutate(target = maxO3) %>%
  select(-maxO3)


# data <- grandile  %>%
#   select(-starts_with("LIB")) %>%
#   mutate(target = REV_DISPONIBLE) %>%
#   select(-IDENT,-REV_DISPONIBLE)



```


L'utilisateur peut ensuite explorer la base  brute dans l'output avec la fonction skim.

```{r}
skim(data)
```


## II - 2 menusubItem : Préparation

### Partition de la base en 3

L'utilisateur choisit la part de la base brute qu'il conserve pour l'entraînement. 
Le reste de la base se séparera équitablement entre la base servant à la validation et celle servant au test. 

```{r}

part_training <- 0.8
part_validation <- (1-part_training)/2

data_split <- initial_validation_split(data,prop = c(part_training,part_validation)) 

train_data <- training(data_split) #le fichier d'entraînement
test_data <- testing(data_split) #le fichier de test
valid_data <- validation(data_split)
train_valid_data <- train_data %>% bind_rows(valid_data)

```

Ces 4 dataframe sont enregistrés comme des variables globales et pourront ainsi être utilisés dans les autres modules.

### Preprocessing : transformation de la base d'entraînement



```{r}

# #Sélection des variables quantis (numériques et integer)
# select_class_df <- function(df,type){
# 
#     names(df[, map_chr(.x = df,.f = class)%in% type])
# 
#     }
# 
# #selection des variables quanti, c'est à dire integer et numeric
# quantis <- select_class_df(data %>% select(-target),c("integer","numeric"))
# 
# 
# #Sélection des variables qualis
# qualis <- select_class_df(data %>% select(-target),c("character"))
# 
# quantis_qualis <- names(data %>% select(-target))

```

Choix des tranformations à apporter à la base brute

```{r}

####Aucune transformation
rec1 <- 
  recipe(target ~ ., data = train_data)

######1seule transfo

#Centrage réduction de toutes les variables quantitatives
rec2 <- rec1 %>%
  step_normalize(all_numeric_predictors()) 

#Imputation des valeurs manquantes avec la moyenne
rec3 <- rec1 %>% 
  step_impute_mean(c(Ne18,Ne15)) 

#enlever des variables (choisir les variables)
rec4 <- rec1 %>% 
  step_rm(T6)

######2transfos (l'ordre importe)

#Imputation des valeurs manquantes PUIS Centrage réduction de toutes les variables 
rec23 <- rec3 %>% 
  step_normalize(all_numeric_predictors())

#centrage réduction de toutes les numériques PUIS imputation des valeurs manquantes
rec32 <- rec2 %>% 
  step_impute_mean(Ne18)

rec <- rec3
rm(rec1,rec2,rec3,rec4,rec23,rec32)
```




Après chaque transformation, l'affichage de la base d'entraînement se met à jour. 
On affiche aussi l'ordre des transformations
Prévoir un bouton pour revenir à la base brute.

```{r}
prep <- prep(rec)
bake <- bake(prep, new_data = NULL)
skim(bake)

```
On affiche les recettes appliquées à la base brute

```{r}
rec
```


## II - 3 Menusubitem : Modèles

Pendant cette phase, on choisit le modèle appliqué à la base.

```{r}

#modèle de regression linéaire
mod_lr <- linear_reg() %>% 
  set_engine("lm") 

#Modèle arbre (avec CART)
mod_tree <- 
  mod_tree <- 
  decision_tree(
    cost_complexity = 0.001,
    tree_depth = 7,
    min_n = NULL
  ) %>%
  set_engine("rpart") %>%
  set_mode("regression")

#random forest
mod_rf <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

#modèle SVM
mod_svm <- svm_linear() %>%
  set_mode("regression") %>%
  set_engine("LiblineaR")


mod <- mod_tree
rm(mod_lr,mod_rf,mod_svm)
```

### Création du workflow incluant les modifications apportées dans la phase précédente

```{r}
wflow <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod)
wflow
```

### Ajustement du modèle sur la base d'entraînement

Attention : la random forest ne marche pas si il y a des valeurs manquantes

```{r}
fit <- wflow %>% fit(data=train_data)

fit %>% 
 extract_fit_parsnip()

```
### Affichage des estimations

```{r}
pred_train <- augment(fit,train_data) %>% select(target,.pred) %>% round(1)
pred_train
```


## II - 4 menuSubitem : Validation

### Validation simple

Affichage des prédictions sur la base de validation

```{r}
pred_valid <- augment(fit,valid_data) %>% select(target,.pred) %>% round(1)
pred_valid
```

### Calcul des indicateurs

```{r}
#Somme des carrés des résidus (RSS : residual sum of squares)
rss <- sum((pred_valid$.pred - pred_valid$target)^2)


#MSE : erreur quadratique moyenne (mean squared error)
#pour enlever l'effet taille (plus la base est grande, plus le rss est grand)
mse <- rss/nrow(pred_valid)


# RSME : racine carré de la MSE, pour se ramener à l'unité de la variable
rmse <- sqrt(mse)
rmse(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric() %>% round(2)

#LRMSE : log de tout ça pour enlever les effets des ordres de grandeur
rmsle <- sqrt(
  (sum(
    (log(pred_valid$.pred +1) - log(pred_valid$target + 1))^2))/(nrow(pred_valid)
                                                                 )
  )


#Coefficient de détermination
rsq <- rsq(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric()

#Coefficient de corrélation linéaire
cor(pred_valid$.pred,pred_valid$target)


#Erreur carré relative
rse <- 1-rsq



tibble(rss,mse,rmse,rmsle,rse,rsq)
rm(rss,mse,rmse,rmsle,rse,rsq)
# mae(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric()
# mape(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric()
```


### Validation croisée

```{r}
# set.seed(345)
nb_folds <- 5
folds <- vfold_cv(train_valid_data,
                  v = nb_folds)#paramétrage de la validation croisée
# 
# Evaluation du modèle avec la validation croisée
metrics <- metric_set(rmse,rsq,mae, mape)
fit_rs <- wflow %>% fit_resamples(folds,
                                  metrics = metrics,
                                  control = control_resamples(save_pred = TRUE))

nb_rs_metrics <- collect_metrics(fit_rs)
nb_rs_predictions <- collect_predictions(fit_rs)


nb_rs_metrics

```


## II - 5 menuSubitem : Généralisation

### Ajustement du modèle retenu sur la base d'entraînement et de validation

Quand l'utilisateur clique sur le bouton, le modèle retenu à l'étape précédente
s'ajuste sur la base d'entraînement et de validation. 

```{r}
fit_final <- 
  wflow %>% 
  fit(train_valid_data)


```

### Affichage du tableau de prédiction sur la base de test

```{r}
pred_testing <- augment(fit_final, test_data) %>% select(target,.pred)
pred_testing
```

### Calcul des indicateurs sur la base de test

```{r}
rss <- sum((pred_valid$.pred - pred_valid$target)^2)
mse <- rss/nrow(pred_valid)
rmse <- sqrt(mse)
rmlse <- sqrt(
  (sum(
    (log(pred_valid$.pred +1) - log(pred_valid$target + 1))^2))/(nrow(pred_valid)
                                                                 )
  )
rsq <- rsq(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric()
rse <- 1-rsq



tibble(rss,mse,rmse,rmlse,rse,rsq)
rm(rss,mse,rmse,rmsle,rse,rsq)
# mae(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric()
# mape(pred_valid,target,.pred) %>% select(.estimate) %>% as.numeric()
```


